{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d8cc08-769e-4889-bf8e-54762ccb6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs  # Importe le module codecs pour travailler avec l'encodage des fichiers texte.\n",
    "import nltk  # Importe le module nltk pour le traitement du langage naturel.\n",
    "import random  # Importe le module random pour générer des nombres aléatoires.\n",
    "import re  # Importe le module re pour travailler avec les expressions régulières.\n",
    "import string  # Importe le module string pour les opérations sur les chaînes de caractères.\n",
    "import csv  # Importe le module csv pour lire et écrire des fichiers CSV.\n",
    "import inflect  # Importe le module inflect pour la conversion de nombres en mots.\n",
    "from count_syllables import count_syllables  # Importe la fonction count_syllables du module count_syllables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78687fd-f856-4963-b6c6-b438f118b884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PoemGenerator(object):\n",
    "    def __init__(self, corpus='buzzfeed_facebook_statues.csv'):\n",
    "        # Initialise la classe PoemGenerator avec un corpus par défaut.\n",
    "        self.only_punctuation = re.compile(r'[^\\w\\s]+$')\n",
    "        # Compile une expression régulière pour trouver la ponctuation uniquement.\n",
    "        self.spaces_and_punctuation = re.compile(r\"[\\w']+|[.,!?;]\")\n",
    "        # Compile une expression régulière pour diviser les mots et la ponctuation.\n",
    "        self.sents = []\n",
    "        # Initialise une liste pour stocker les phrases du corpus.\n",
    "        self.words = []\n",
    "        # Initialise une liste pour stocker les mots du corpus.\n",
    "        self.all_words = []\n",
    "        # Initialise une liste pour stocker tous les mots (sans ponctuation) du corpus.\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        # Initialise le moteur d'inflection pour la conversion de nombres en mots.\n",
    "        self.read_corpus(corpus)\n",
    "        # Appelle la méthode read_corpus pour charger le corpus spécifié.\n",
    "        self.bigrams = list(nltk.bigrams(self.words))\n",
    "        # Crée une liste de bigrammes à partir des mots du corpus.\n",
    "        self.cfd = nltk.ConditionalFreqDist(self.bigrams)\n",
    "        # Initialise une fréquence conditionnelle des bigrammes.\n",
    "        self.history = []\n",
    "        # Initialise une liste pour suivre l'historique des générations de poèmes.\n",
    "\n",
    "    def read_corpus(self, corpus):\n",
    "        \"\"\"Given filename of corpus, populate words, all_words, and sents.\"\"\"\n",
    "        if corpus.endswith('.csv'):\n",
    "            if 'buzzfeed_facebook_statuses' in corpus:\n",
    "                return self.read_buzzfeed_corpus(corpus)\n",
    "            else:\n",
    "                return self.read_csv_corpus(corpus)\n",
    "        elif corpus.endswith('.txt'):\n",
    "            return self.read_txt_corpus(corpus)\n",
    "        else:\n",
    "            raise TypeError(('Unrecognized corpus file type: %s.' % corpus) +\n",
    "                            '\".txt\" and \".csv\" are only supported')\n",
    "        # Méthode pour lire le corpus en fonction du type de fichier spécifié.\n",
    "\n",
    "    def read_txt_corpus(self, corpus):\n",
    "        with codecs.open(corpus, 'r', 'utf-8') as corpus_content:\n",
    "            text = corpus_content.read()\n",
    "            sents = nltk.tokenize.sent_tokenize(text)\n",
    "            words = nltk.tokenize.word_tokenize(text)\n",
    "            self.sents.extend(sents)\n",
    "            self.words.extend(words)\n",
    "            self.all_words.extend([word for word in words\n",
    "                                   if not\n",
    "                                   self.only_punctuation.match(word)])\n",
    "        # Méthode pour lire un fichier texte, diviser en phrases et mots, et les ajouter aux listes appropriées.\n",
    "\n",
    "    def read_buzzfeed_corpus(self, corpus):\n",
    "        with open(corpus, newline='', encoding='utf-8') as statuses:\n",
    "            reader = csv.reader(statuses, delimiter=',')\n",
    "            for row in reader:\n",
    "                if 'via buzzfeed ' not in row[1].lower():  # only English\n",
    "                    # split title into a list of words and punctuation\n",
    "                    title = self.spaces_and_punctuation.findall(row[2])\n",
    "                    # spell out digits into ordinal words for syllable counting\n",
    "                    title = [string.capwords(\n",
    "                             self.inflect_engine.number_to_words(int(word)))\n",
    "                             if word.isdigit() else word for word in title]\n",
    "                    self.sents.append(title)\n",
    "                    self.words.extend(title)\n",
    "                    self.all_words.extend([word for word in title\n",
    "                                           if not\n",
    "                                           self.only_punctuation.match(word)])\n",
    "        # Méthode pour lire un fichier CSV spécifique à BuzzFeed, extraire les titres, et les ajouter aux listes appropriées.\n",
    "\n",
    "    def markov(self, word, n):\n",
    "        if n > 0:\n",
    "            print(word,)\n",
    "            n = n - 1\n",
    "            self.markov(random.choice(self.cfd[word].items())[0], n)\n",
    "        else:\n",
    "            print('')\n",
    "        # Méthode pour générer du texte à l'aide d'une chaîne de Markov avec une profondeur spécifiée.\n",
    "\n",
    "    def generate_text(self):\n",
    "        word = random.choice(self.bigrams)[0]\n",
    "        self.markov(word, 15)\n",
    "        # Méthode pour générer du texte en utilisant la méthode markov avec une longueur spécifiée.\n",
    "\n",
    "    def haiku_line(self, line, current_syllables, next_words, target_syllables):\n",
    "        if next_words == []:\n",
    "            # this branch failed\n",
    "            return None\n",
    "        else:\n",
    "            word = random.choice(next_words)\n",
    "        new_line = line[:]\n",
    "        new_line.append(word)\n",
    "        new_syllables = sum(map(count_syllables, new_line))\n",
    "        if new_syllables == target_syllables:\n",
    "            return new_line\n",
    "        elif new_syllables > target_syllables:\n",
    "            new_next_words = next_words[:]\n",
    "            new_next_words.remove(word)\n",
    "            return self.haiku_line(line, current_syllables, new_next_words, target_syllables)\n",
    "        else:\n",
    "            new_next_words = [freq[0] for freq in self.cfd[word].items()\n",
    "                              if not self.only_punctuation.match(freq[0])]\n",
    "            branch = self.haiku_line(new_line, new_syllables, new_next_words, target_syllables)\n",
    "            if branch:\n",
    "                return branch\n",
    "            else:\n",
    "                new_next_words = next_words[:]\n",
    "                new_next_words.remove(word)\n",
    "                return self.haiku_line(line, current_syllables, new_next_words, target_syllables)\n",
    "        # Méthode pour générer une ligne de haïku en utilisant la récursivité pour atteindre le nombre de syllabes cible.\n",
    "\n",
    "    def generate_haiku(self):\n",
    "        haiku = ''\n",
    "        first = self.haiku_line([], 0, self.all_words, 5)\n",
    "        haiku = haiku + ' '.join(first) + '\\n'\n",
    "        next_words = [freq[0] for freq in self.cfd[first[-1]].items()\n",
    "                      if not self.only_punctuation.match(freq[0])]\n",
    "        if not next_words:\n",
    "            next_words = self.all_words\n",
    "        second = self.haiku_line([], 0, next_words, 7)\n",
    "        haiku = haiku + ' '.join(second) + '\\n'\n",
    "        next_words = [freq[0] for freq in self.cfd[second[-1]].items()\n",
    "                      if not self.only_punctuation.match(freq[0])]\n",
    "        if not next_words:\n",
    "            next_words = self.all_words\n",
    "        third = self.haiku_line([], 0, next_words, 5)\n",
    "        haiku = haiku + ' '.join(third) + '\\n'\n",
    "        return haiku\n",
    "    # Méthode pour générer un haïku en appelant la méthode haiku_line trois fois pour les trois lignes.\n",
    "\n",
    "    def generate_endless_poem(self, previous_line):\n",
    "        random_syllables = random.choice(range(1, 26))\n",
    "        if previous_line is None:\n",
    "            next = self.haiku_line([], 0, self.all_words, random_syllables)\n",
    "            print(' '.join(next))\n",
    "        else:\n",
    "            next_words = [freq[0] for freq in self.cfd[previous_line[-1]].items()\n",
    "                          if not self.only_punctuation.match(freq[0])]\n",
    "            next = self.haiku_line([], 0, next_words, random_syllables)\n",
    "            print(' '.join(next))\n",
    "        self.generate_endless_poem(next)\n",
    "    # Méthode pour générer un poème sans fin en utilisant la méthode haiku_line avec une ligne précédente comme point de départ.\n",
    "\n",
    "    def generate_sonnet(self):\n",
    "        sonnet = \"\"\n",
    "        # Quatrains (4 quatrains)\n",
    "        for _ in range(4):\n",
    "            line1 = self.haiku_line([], 0, self.all_words, 10)  # 10 syllables per line\n",
    "            line2 = self.haiku_line([], 0, self.all_words, 10)\n",
    "            line3 = self.haiku_line([], 0, self.all_words, 10)\n",
    "            line4 = self.haiku_line([], 0, self.all_words, 10)\n",
    "            sonnet += ' '.join(line1) + '\\n'\n",
    "            sonnet += ' '.join(line2) + '\\n'\n",
    "            sonnet += ' '.join(line3) + '\\n'\n",
    "            sonnet += ' '.join(line4) + '\\n'\n",
    "\n",
    "        # Distiche (2 vers)\n",
    "        line5 = self.haiku_line([], 0, self.all_words, 10)\n",
    "        line6 = self.haiku_line([], 0, self.all_words, 10)\n",
    "        sonnet += ' '.join(line5) + '\\n'\n",
    "        sonnet += ' '.join(line6) + '\\n'\n",
    "\n",
    "        return sonnet\n",
    "    # Méthode pour générer un sonnet en utilisant la méthode haiku_line pour chaque ligne du sonnet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42fc753-22f3-400b-b19b-2d37ef0628cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King Is Coming Back\n",
      "With Spaghetti And Kourtney\n",
      "Kardashian Used Art\n",
      "\n",
      "============================== \n",
      "\n",
      "By Surprising Dance Moves What Teens Say It\n",
      "She Went For Greatest Movies Told The Week\n",
      "Around Your Drink Of Social Media\n",
      "In Ohio Man Arrested And Jay\n",
      "The Prisoners The Miley Cyrus' VMA\n",
      "Chris Kelly Ignition Remix Is Six\n",
      "Renaissance Baby We've Tasted Them When\n",
      "Photos McDonald's Was Headed Towards\n",
      "Sixteen Eyebrow Diagrams To Visit\n",
      "Could Hope From Mad A Pic A Fox News Blurred\n",
      "Baby Cub Scare Prank How Old Dancers in\n",
      "You McDonald's Or Alan Gross Photos\n",
      "Phone In Mary Kate Ashley Have Starred On\n",
      "Under 5'4 Twenty-nine Puntastic Jokes\n",
      "Dyed Hair Will Perform Salt N Pepa And\n",
      "The Giant Camel Crucial Booze This Dog\n",
      "Shop Seventeen Razones por frases de\n",
      "Every Bra Seventeen Tips From MTV\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    generator = PoemGenerator(corpus='buzzfeed_facebook_statuses.csv')\n",
    "    haiku = generator.generate_haiku()\n",
    "    sonnet = generator.generate_sonnet()\n",
    "    print(haiku)\n",
    "    print('=' * 30, '\\n')\n",
    "    print(sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66ad4cb4-af9b-4205-b55c-edbf22bdc4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Miss America You Chris\n",
      "Pratt's\n",
      "Is Arrested Is Blue Dress Was Downed Malaysia Airlines Sends Her\n",
      "Husband's Suspension From\n",
      "Spending Ten Lovely Version Of Lana Del Rey's New\n",
      "Pregnancy And Gay This Incredibly Touching\n",
      "Photos Guaranteed To Listen To Wait\n",
      "Archie Picks Betty PIC Extreme Oreo Got One Hundred And Forty-two Hugs That Seem\n",
      "What's At Swimming Naked Together Can Pull This\n",
      "Ink Blot Test In Forty-seven Brides For Oh My Body Twenty-seven Heart Twenty-three\n",
      "Completely Geek Out Twenty-three Gadgets That Never Make His Oscars Performance These Cult Films\n",
      "This Message To Join Augusta National High With Secretly Attracts You Happy The\n",
      "Thirty Look at Play F k Is Running Out Grumpy Cat Why This Zombie Apocalypse College\n",
      "Major Announcement Photo Ninety-four Reasons Not What Fictional City\n",
      "Chiefs The RNC Campbell's Without Your Imaginary World Twenty-eight Pictures One Thing You've Already\n",
      "Testing Its Time Taylor Swift's Fight Ebola\n",
      "Had Names Say She's\n",
      "Homeless Eleven Struggles Everyone Who Sometimes Life Why\n",
      "Killing\n",
      "Elephants You Win At Nasty Gal Twenty-seven Of Toy\n",
      "Company Slogans That Changes Everything you realize all your\n",
      "pet a Meme\n",
      "Pics Of Renée Zellweger Looks Unbelievably Awkward Fifteen YA Novels To Hack Lena Dunham\n",
      "Dancing In April Fools' Prank The Cast For Breakfast\n",
      "Twenty-seven\n",
      "Gendered For AC DC\n",
      "The Furious Cast Members Wear Teenagers Are Totally Backfired Your Shirts\n",
      "What Anti Instagram Patrick Helps Little Wrecking Ball Back with the\n",
      "United States Twenty-one Most Perfectly Summed Up Letter As You Missed\n",
      "From\n",
      "Maine Would Ruin The Release Second Child Has Uttered Thirty Cute Products You Heard\n",
      "In Police Department Uses\n",
      "For Buying It With Good On Live Alone Is Adorably Shares Your Soul Republicans\n",
      "Want My Job Is Zac Efron And Laughably Bad Here's The\n",
      "Union That Won't Last Thirty-five\n",
      "Year Twenty-one Flawless Human History Crazy Boston Common Medicines You Guess How\n",
      "Twitter Black People Used Her Cardboard Cutout Of Sense\n",
      "Twenty-two Rebeldes sin causa del siglo XXI Nineteen Phrases I\n",
      "Still Challenging Instagram Filter Over And\n",
      "Single Most WTF\n",
      "Is Healthy\n",
      "Cory Monteith's Death A Bubble Boy Was Set Top Thirty-five White Shark Tank Of\n",
      "Makeup Looks On His Twin\n",
      "Polar Bears\n",
      "That Shoot\n",
      "Twenty-one Very Valuable Lesson Twenty-nine Gifs de actuar\n",
      "This Adorable Sesame Street Foods Your Kids Thirty-seven Easy Way Easier\n",
      "Hash Oil Spill Their Meat The Hold Off Nineteen\n",
      "mejores posts de Yo Soy Betty La prueba definitiva\n",
      "de México Twenty Embarassing Lifehacks That Logged All If Dwight The Teams The It\n",
      "Rains Tom Brady Posts a District campaign against AIDS Day Nirvana Smells You Seventeen\n",
      "Pretty Great Dane Throw It OK Go Twenty-two TV Duo Slays\n",
      "Dance In Missouri Reportedly Introducing BuzzFeed\n",
      "Don't Contain\n",
      "Any Parent's Life As Part Of Leonardo DiCaprio's Beard\n",
      "Instagram After Paul Gosselaar You'll Buy Today How Good\n",
      "Which Bridesmaids Thirteen Valentines For Future And Gravy Twenty momentos que os pais e foi\n",
      "en el pueblo de antes y Su\n",
      "Pandilla Twenty-four Rules Everyone Made Things\n",
      "Rachel Maddow The Scientists Who Exposed David Letterman In Memoriam Why Adelaide Is\n",
      "Perfectly Sum Up Understand When To Whole\n",
      "Foods To Sell Cookies That Normal Humans Of Narnia\n",
      "Look Cool\n",
      "Etsy You Have\n",
      "Extreme Is Blowing Facts Than One\n",
      "Thing Happened When\n",
      "They're\n",
      "People\n",
      "Bought A Romantic Comedy Top Secret With His Dancing Of Hobby\n",
      "Keeping Me Find Most Disrespectful Things Found Your In Olympic Gymnastics Gold What\n",
      "It's Ridiculously\n",
      "Good BuzzFeed Viral Sexting on June GIFs\n",
      "Guaranteed To Platinum Blonde Is Based On Camera Harassing Teenage Alt Rock Band Are\n",
      "Showing\n",
      "Off The Kinky Acronym Pics Of\n",
      "Disney Female Cartoonists On Current\n",
      "Events And David Hasselhoff's New Clean All Chopped Off Americans Are Constantly Bombarded With\n",
      "Cheese On All\n",
      "Went For Game\n",
      "Show Another Side Eye Music Eleven Service Dog Is Real Twenty-three Beard\n",
      "Must Know Bridesmaids Character Are Hilariously Describe Women Less Than\n",
      "Manbuns Nineteen Hidden Camera And Beautiful Man Told\n",
      "Through Finding Instagram For\n",
      "Food BuzzFeed Business At UVA And\n",
      "Hilarious Mad That James\n",
      "To Model Of Gin\n",
      "And Wanted For Him Use When She's Used This\n",
      "Office's Cat Ariana Grande IS TODAY Questions\n",
      "Men Bieber New Life Doppelgänger\n",
      "Chad\n",
      "Smith Deserve A Fat Shaming After Finding Instagram Twenty-one Five star\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only join an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m endless_poem \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_endless_poem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(endless_poem)\n",
      "Cell \u001b[1;32mIn[7], line 143\u001b[0m, in \u001b[0;36mPoemGenerator.generate_endless_poem\u001b[1;34m(self, previous_line)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mnext\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhaiku_line([], \u001b[38;5;241m0\u001b[39m, next_words, random_syllables)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mnext\u001b[39m))\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_endless_poem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 143\u001b[0m, in \u001b[0;36mPoemGenerator.generate_endless_poem\u001b[1;34m(self, previous_line)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mnext\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhaiku_line([], \u001b[38;5;241m0\u001b[39m, next_words, random_syllables)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mnext\u001b[39m))\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_endless_poem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping similar frames: PoemGenerator.generate_endless_poem at line 143 (88 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[7], line 143\u001b[0m, in \u001b[0;36mPoemGenerator.generate_endless_poem\u001b[1;34m(self, previous_line)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mnext\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhaiku_line([], \u001b[38;5;241m0\u001b[39m, next_words, random_syllables)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mnext\u001b[39m))\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_endless_poem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 142\u001b[0m, in \u001b[0;36mPoemGenerator.generate_endless_poem\u001b[1;34m(self, previous_line)\u001b[0m\n\u001b[0;32m    139\u001b[0m     next_words \u001b[38;5;241m=\u001b[39m [freq[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m freq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfd[previous_line[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    140\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monly_punctuation\u001b[38;5;241m.\u001b[39mmatch(freq[\u001b[38;5;241m0\u001b[39m])]\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mnext\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhaiku_line([], \u001b[38;5;241m0\u001b[39m, next_words, random_syllables)\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_endless_poem(\u001b[38;5;28mnext\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only join an iterable"
     ]
    }
   ],
   "source": [
    "endless_poem = generator.generate_endless_poem(None)\n",
    "print(endless_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb18bc5-fcdb-4d05-812f-5b4b9ca1b4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poemgen",
   "language": "python",
   "name": "poemgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
